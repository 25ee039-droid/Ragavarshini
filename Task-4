# ---------------------------------------------------
# SPAM SMS DETECTION PROJECT
# ---------------------------------------------------
# Imagine youâ€™re sorting your text messages:
# Some are friendly chats (ham), others are annoying ads (spam).
# Our job: teach a computer to tell them apart.
# ---------------------------------------------------

# Step 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

# ---------------------------------------------------
# Step 2: Load dataset
# ---------------------------------------------------
# Example dataset: "spam.csv" (SMS Spam Collection from UCI)
data = pd.read_csv("spam.csv", encoding="latin-1")[['v1','v2']]
data.columns = ['label','message']

print("First look at the data:")
print(data.head())

# ---------------------------------------------------
# Step 3: Preprocess data
# ---------------------------------------------------
# Convert labels to binary (spam=1, ham=0)
data['label'] = data['label'].map({'ham':0, 'spam':1})

X = data['message']
y = data['label']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Convert text into numbers using TF-IDF
# TF-IDF highlights important words like "free", "win", "offer"
tfidf = TfidfVectorizer(stop_words='english', max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# ---------------------------------------------------
# Step 4: Train models
# ---------------------------------------------------
# Naive Bayes: quick guesser based on word probabilities
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)
y_pred_nb = nb.predict(X_test_tfidf)

# Logistic Regression: draws a line between spam and ham
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_tfidf, y_train)
y_pred_log = log_reg.predict(X_test_tfidf)

# Support Vector Machine: finds the best boundary
svm = LinearSVC()
svm.fit(X_train_tfidf, y_train)
y_pred_svm = svm.predict(X_test_tfidf)

# ---------------------------------------------------
# Step 5: Evaluate models
# ---------------------------------------------------
print("\nNaive Bayes Results:")
print(classification_report(y_test, y_pred_nb))
print("Accuracy:", accuracy_score(y_test, y_pred_nb))

print("\nLogistic Regression Results:")
print(classification_report(y_test, y_pred_log))
print("Accuracy:", accuracy_score(y_test, y_pred_log))

print("\nSupport Vector Machine Results:")
print(classification_report(y_test, y_pred_svm))
print("Accuracy:", accuracy_score(y_test, y_pred_svm))

# ---------------------------------------------------
# Step 6: Confusion Matrices
# ---------------------------------------------------
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, model_name, y_pred in zip(
    axes,
    ["Naive Bayes", "Logistic Regression", "SVM"],
    [y_pred_nb, y_pred_log, y_pred_svm],
):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_title(f"{model_name} Confusion Matrix")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")

plt.tight_layout()
plt.show()
